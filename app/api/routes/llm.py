"""
LLM æ™ºèƒ½åˆ†æ API è·¯ç”± (v0.3.0)
åŸºäº LangChain çš„é«˜çº§ AI åˆ†æå’Œå¯¹è¯æŸ¥è¯¢æ¥å£
"""

from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.core.auth_simple import get_current_user
from app.core.logger import get_logger
from app.models.subscription import User
from app.services.llm_service import LLMService
from app.services.websocket_service import websocket_service
from app.collectors.github_collector import GitHubCollector

logger = get_logger(__name__)
router = APIRouter(prefix="/llm", tags=["AIæ™ºèƒ½åˆ†æ"])

# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()


class ChatRequest(BaseModel):
    """å¯¹è¯è¯·æ±‚æ¨¡å‹"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    context_data: Optional[Dict[str, Any]] = Field(None, description="ä¸Šä¸‹æ–‡æ•°æ®")
    stream: bool = Field(False, description="æ˜¯å¦æµå¼è¾“å‡º")


class ChatResponse(BaseModel):
    """å¯¹è¯å“åº”æ¨¡å‹"""
    response: str = Field(..., description="AIå›å¤")
    conversation_id: str = Field(..., description="å¯¹è¯ID")
    timestamp: str = Field(..., description="æ—¶é—´æˆ³")


class AnalysisRequest(BaseModel):
    """åˆ†æè¯·æ±‚æ¨¡å‹"""
    repository: str = Field(..., description="ä»“åº“åç§°ï¼Œæ ¼å¼ï¼šowner/repo")
    analysis_type: str = Field(default="comprehensive", description="åˆ†æç±»å‹")
    timeframe: str = Field(default="30d", description="æ—¶é—´èŒƒå›´")


class AnalysisResponse(BaseModel):
    """åˆ†æå“åº”æ¨¡å‹"""
    analysis: Dict[str, Any] = Field(..., description="åˆ†æç»“æœ")
    repository: str = Field(..., description="ä»“åº“åç§°")
    analysis_type: str = Field(..., description="åˆ†æç±»å‹")
    generated_at: str = Field(..., description="ç”Ÿæˆæ—¶é—´")


class SmartSummaryRequest(BaseModel):
    """æ™ºèƒ½æ‘˜è¦è¯·æ±‚æ¨¡å‹"""
    repository: str = Field(..., description="ä»“åº“åç§°")
    timeframe: str = Field(default="weekly", description="æ—¶é—´èŒƒå›´")
    days: int = Field(default=7, description="å¤©æ•°")


class SearchRequest(BaseModel):
    """æœç´¢åˆ†æè¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    context_data: Optional[Dict[str, Any]] = Field(None, description="ä¸Šä¸‹æ–‡æ•°æ®")


@router.post("/chat", response_model=ChatResponse)
async def chat_with_ai(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """ä¸AIè¿›è¡Œå¯¹è¯"""
    try:
        user_id = str(current_user.id)
        
        # æ™®é€šå¯¹è¯æ¨¡å¼
        if not request.stream:
            response = await llm_service.chat_with_context(
                user_id=user_id,
                message=request.message,
                context_data=request.context_data
            )
            
            # å‘é€å¯¹è¯é€šçŸ¥
            background_tasks.add_task(
                websocket_service.send_ai_insight_notification,
                {
                    "type": "chat_response",
                    "message": request.message,
                    "response": response,
                    "conversation_id": user_id
                },
                current_user.id
            )
            
            return ChatResponse(
                response=response,
                conversation_id=user_id,
                timestamp=datetime.now().isoformat()
            )
        
        else:
            # æµå¼å¯¹è¯æ¨¡å¼
            return StreamingResponse(
                stream_chat_response(user_id, request.message, request.context_data),
                media_type="text/plain"
            )
            
    except Exception as e:
        import traceback
        logger.error(f"ğŸ’¥ AIå¯¹è¯å¤±è´¥: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"AIå¯¹è¯å¤±è´¥: {str(e)}"
        )


async def stream_chat_response(user_id: str, message: str, context_data: Optional[Dict[str, Any]]):
    """æµå¼å¯¹è¯å“åº”ç”Ÿæˆå™¨"""
    try:
        response_tokens = []
        
        async def token_callback(token: str):
            response_tokens.append(token)
            yield f"data: {token}\n\n"
        
        # æ‰§è¡Œæµå¼å¯¹è¯
        await llm_service.chat_with_context(
            user_id=user_id,
            message=message,
            context_data=context_data,
            stream_callback=token_callback
        )
        
        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"ğŸ’¥ æµå¼å¯¹è¯å¤±è´¥: {e}")
        yield f"data: é”™è¯¯: {str(e)}\n\n"


@router.post("/analyze", response_model=AnalysisResponse)
async def analyze_repository(
    request: AnalysisRequest,
    current_user: User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """æ™ºèƒ½åˆ†æä»“åº“"""
    try:
        # è§£æä»“åº“åç§°
        if "/" not in request.repository:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ä»“åº“åç§°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º owner/repo"
            )
        
        owner, repo = request.repository.split("/", 1)
        
        # æ”¶é›†ä»“åº“æ•°æ®
        github_collector = GitHubCollector()
        repo_data = await github_collector.collect_repository_data(owner, repo)
        
        # æ‰§è¡Œæ™ºèƒ½åˆ†æ
        analysis_result = await llm_service.analyze_repository_intelligence(
            repo_data=repo_data,
            analysis_type=request.analysis_type
        )
        
        if "error" in analysis_result:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=analysis_result["error"]
            )
        
        # å‘é€åˆ†æç»“æœé€šçŸ¥
        background_tasks.add_task(
            websocket_service.send_ai_insight_notification,
            {
                "type": "repository_analysis",
                "repository": request.repository,
                "analysis_type": request.analysis_type,
                "analysis": analysis_result
            },
            current_user.id
        )
        
        return AnalysisResponse(
            analysis=analysis_result,
            repository=request.repository,
            analysis_type=request.analysis_type,
            generated_at=datetime.now().isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ’¥ ä»“åº“åˆ†æå¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ä»“åº“åˆ†æå¤±è´¥: {str(e)}"
        )


@router.post("/smart-summary")
async def generate_smart_summary(
    request: SmartSummaryRequest,
    current_user: User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
    try:
        # è§£æä»“åº“åç§°
        if "/" not in request.repository:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ä»“åº“åç§°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º owner/repo"
            )
        
        owner, repo = request.repository.split("/", 1)
        
        # æ”¶é›†æ´»åŠ¨æ•°æ®
        github_collector = GitHubCollector()
        end_date = datetime.now()
        start_date = end_date - timedelta(days=request.days)
        
        activities = await github_collector.collect_activities(
            owner, repo, start_date, end_date
        )
        
        # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
        summary_result = await llm_service.generate_smart_summary(
            activities=activities,
            timeframe=request.timeframe
        )
        
        if "error" in summary_result:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=summary_result["error"]
            )
        
        # å‘é€æ‘˜è¦é€šçŸ¥
        background_tasks.add_task(
            websocket_service.send_ai_insight_notification,
            {
                "type": "smart_summary",
                "repository": request.repository,
                "timeframe": request.timeframe,
                "summary": summary_result
            },
            current_user.id
        )
        
        return summary_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ’¥ æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
        )


@router.post("/search")
async def search_and_analyze(
    request: SearchRequest,
    current_user: User = Depends(get_current_user)
):
    """æœç´¢å¹¶åˆ†æ"""
    try:
        result = await llm_service.search_and_analyze(
            query=request.query,
            context_data=request.context_data
        )
        
        if "error" in result:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=result["error"]
            )
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ’¥ æœç´¢åˆ†æå¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æœç´¢åˆ†æå¤±è´¥: {str(e)}"
        )


@router.delete("/conversation")
async def clear_conversation(
    current_user: User = Depends(get_current_user)
):
    """æ¸…é™¤å¯¹è¯å†å²"""
    try:
        user_id = str(current_user.id)
        success = await llm_service.clear_conversation(user_id)
        
        if success:
            return {"message": "å¯¹è¯å†å²å·²æ¸…é™¤", "user_id": user_id}
        else:
            return {"message": "æœªæ‰¾åˆ°å¯¹è¯å†å²", "user_id": user_id}
            
    except Exception as e:
        logger.error(f"ğŸ’¥ æ¸…é™¤å¯¹è¯å†å²å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¸…é™¤å¯¹è¯å†å²å¤±è´¥: {str(e)}"
        )


@router.get("/status")
async def get_llm_status(
    current_user: User = Depends(get_current_user)
):
    """è·å–LLMæœåŠ¡çŠ¶æ€"""
    try:
        status_info = llm_service.get_service_status()
        return status_info
        
    except Exception as e:
        logger.error(f"ğŸ’¥ è·å–LLMçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–LLMçŠ¶æ€å¤±è´¥: {str(e)}"
        )


@router.post("/batch-analyze")
async def batch_analyze_repositories(
    repositories: List[str],
    analysis_type: str = "comprehensive",
    current_user: User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """æ‰¹é‡åˆ†æå¤šä¸ªä»“åº“"""
    try:
        if len(repositories) > 10:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ‰¹é‡åˆ†ææœ€å¤šæ”¯æŒ10ä¸ªä»“åº“"
            )
        
        results = []
        github_collector = GitHubCollector()
        
        for repo in repositories:
            try:
                if "/" not in repo:
                    results.append({
                        "repository": repo,
                        "error": "ä»“åº“åç§°æ ¼å¼é”™è¯¯"
                    })
                    continue
                
                owner, repo_name = repo.split("/", 1)
                
                # æ”¶é›†ä»“åº“æ•°æ®
                repo_data = await github_collector.collect_repository_data(owner, repo_name)
                
                # æ‰§è¡Œåˆ†æ
                analysis_result = await llm_service.analyze_repository_intelligence(
                    repo_data=repo_data,
                    analysis_type=analysis_type
                )
                
                results.append({
                    "repository": repo,
                    "analysis": analysis_result,
                    "status": "success"
                })
                
            except Exception as e:
                results.append({
                    "repository": repo,
                    "error": str(e),
                    "status": "failed"
                })
        
        # å‘é€æ‰¹é‡åˆ†æå®Œæˆé€šçŸ¥
        background_tasks.add_task(
            websocket_service.send_ai_insight_notification,
            {
                "type": "batch_analysis_complete",
                "repositories": repositories,
                "analysis_type": analysis_type,
                "results_count": len(results),
                "success_count": len([r for r in results if r.get("status") == "success"])
            },
            current_user.id
        )
        
        return {
            "results": results,
            "total": len(repositories),
            "success": len([r for r in results if r.get("status") == "success"]),
            "failed": len([r for r in results if r.get("status") == "failed"]),
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ’¥ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}"
        ) 