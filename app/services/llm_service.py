"""
LLM æ™ºèƒ½åˆ†ææœåŠ¡ (v0.3.0)
åŸºäº LangChain æ¡†æ¶ï¼Œæä¾›é«˜çº§ AI åˆ†æå’Œå¯¹è¯æŸ¥è¯¢åŠŸèƒ½
ä¸åŸæœ‰ ai_service å¹¶å­˜ï¼Œæä¾›æ›´å¼ºå¤§çš„æ™ºèƒ½åˆ†æèƒ½åŠ›
"""

import json
from datetime import datetime
from typing import Dict, List, Any, Optional

from langchain.chains import LLMChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.callbacks import AsyncCallbackHandler
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

from app.core.config import get_settings
from app.core.logger import get_logger

logger = get_logger(__name__)


class StreamingCallbackHandler(AsyncCallbackHandler):
    """æµå¼è¾“å‡ºå›è°ƒå¤„ç†å™¨"""
    
    def __init__(self, callback_func=None):
        self.callback_func = callback_func
        self.tokens = []
    
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """å¤„ç†æ–°çš„token"""
        if self.callback_func:
            await self.callback_func(token)
        self.tokens.append(token)


class LLMService:
    """åŸºäº LangChain çš„é«˜çº§ LLM æœåŠ¡"""
    
    def __init__(self):
        self.settings = get_settings()
        self.ai_config = self.settings.ai
        
        # åˆå§‹åŒ– LangChain ç»„ä»¶
        self.llm = None
        self.conversation_chains = {}  # ç”¨æˆ·ä¼šè¯é“¾
        self.search_tool = DuckDuckGoSearchRun()
        
        self._initialize_llm()
        
    def _initialize_llm(self):
        """åˆå§‹åŒ– LLM æ¨¡å‹"""
        try:
            if self.ai_config.provider == "openai" and self.ai_config.openai_api_key:
                self.llm = ChatOpenAI(
                    model=self.ai_config.openai_model,
                    api_key=self.ai_config.openai_api_key,
                    temperature=self.ai_config.temperature,
                    max_tokens=self.ai_config.max_tokens,
                    streaming=True
                )
                logger.info(f"âœ… LangChain LLM åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {self.ai_config.openai_model}")
            else:
                logger.warning("âš ï¸ æœªé…ç½® OpenAI API Keyï¼ŒLLM æœåŠ¡å°†ä¸å¯ç”¨")
        except Exception as e:
            logger.error(f"ğŸ’¥ LLM åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def get_conversation_chain(self, user_id: str):
        """è·å–æˆ–åˆ›å»ºç”¨æˆ·çš„å¯¹è¯é“¾"""
        if user_id not in self.conversation_chains:
            from langchain.chains import LLMChain
            from langchain.memory import ConversationBufferWindowMemory
            from langchain.prompts import PromptTemplate
            
            # ä½¿ç”¨ç®€åŒ–çš„æç¤ºæ¨¡æ¿
            template = """ä½ æ˜¯ GitHubSentinel çš„ AI åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åˆ†æ GitHub ä»“åº“æ´»åŠ¨å’Œä»£ç å¼€å‘è¶‹åŠ¿ã€‚

ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. åˆ†æä»“åº“æ´»åŠ¨æ•°æ®ï¼Œæä¾›æ·±åº¦æ´å¯Ÿ
2. å›ç­”ç”¨æˆ·å…³äºä»£ç ã€æäº¤ã€Issueã€PR ç­‰çš„é—®é¢˜
3. æä¾›å¼€å‘å»ºè®®å’Œæœ€ä½³å®è·µ
4. è§£é‡ŠæŠ€æœ¯æ¦‚å¿µå’Œä»£ç æ¨¡å¼

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šã€å‹å¥½çš„è¯­è°ƒã€‚å¦‚æœéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·ä¸»åŠ¨è¯¢é—®ã€‚

å½“å‰å¯¹è¯å†å²:
{history}

ç”¨æˆ·é—®é¢˜: {input}
AIå›ç­”:"""

            prompt = PromptTemplate(
                input_variables=["history", "input"],
                template=template
            )
            
            memory = ConversationBufferWindowMemory(
                k=10,  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
                memory_key="history",
                input_key="input"
            )
            
            self.conversation_chains[user_id] = LLMChain(
                llm=self.llm,
                prompt=prompt,
                memory=memory,
                verbose=True
            )
            
        return self.conversation_chains[user_id]
    
    async def chat_with_context(
        self, 
        user_id: str, 
        message: str, 
        context_data: Optional[Dict[str, Any]] = None,
        stream_callback: Optional[callable] = None
    ) -> str:
        """ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œå¯ä»¥åŒ…å«ä¸Šä¸‹æ–‡æ•°æ®"""
        try:
            if not self.llm:
                return "æŠ±æ­‰ï¼ŒAI æœåŠ¡å½“å‰ä¸å¯ç”¨ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚"
            
            # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æ¶ˆæ¯
            enhanced_message = message
            if context_data:
                context_str = self._format_context_data(context_data)
                enhanced_message = f"""
ä¸Šä¸‹æ–‡æ•°æ®ï¼š
{context_str}

ç”¨æˆ·é—®é¢˜ï¼š{message}
"""
            
            # è·å–å¯¹è¯é“¾
            chain = self.get_conversation_chain(user_id)
            
            # è®¾ç½®æµå¼å›è°ƒ
            callbacks = []
            if stream_callback:
                callbacks.append(StreamingCallbackHandler(stream_callback))
            
            # æ‰§è¡Œå¯¹è¯
            response = await chain.arun(
                input=enhanced_message,
                callbacks=callbacks
            )
            
            logger.info(f"âœ… LLM å¯¹è¯å®Œæˆ - ç”¨æˆ·: {user_id}")
            return response
            
        except Exception as e:
            logger.error(f"ğŸ’¥ LLM å¯¹è¯å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    async def analyze_repository_intelligence(
        self, 
        repo_data: Dict[str, Any],
        analysis_type: str = "comprehensive"
    ) -> Dict[str, Any]:
        """æ™ºèƒ½åˆ†æä»“åº“æ•°æ®"""
        try:
            if not self.llm:
                return {"error": "AI æœåŠ¡ä¸å¯ç”¨"}
            
            # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©ä¸åŒçš„æç¤ºè¯
            if analysis_type == "comprehensive":
                analysis_result = await self._comprehensive_analysis(repo_data)
            elif analysis_type == "security":
                analysis_result = await self._security_analysis(repo_data)
            elif analysis_type == "performance":
                analysis_result = await self._performance_analysis(repo_data)
            elif analysis_type == "quality":
                analysis_result = await self._quality_analysis(repo_data)
            else:
                analysis_result = await self._comprehensive_analysis(repo_data)
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ æ™ºèƒ½åˆ†æå¤±è´¥: {e}")
            return {"error": f"åˆ†æå¤±è´¥: {str(e)}"}
    
    async def _comprehensive_analysis(self, repo_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç»¼åˆåˆ†æ"""
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ä»£ç æ¶æ„å¸ˆå’Œé¡¹ç›®åˆ†æä¸“å®¶ã€‚è¯·å¯¹æä¾›çš„ GitHub ä»“åº“æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æã€‚

åˆ†æç»´åº¦åŒ…æ‹¬ï¼š
1. ä»£ç è´¨é‡å’Œæ¶æ„è®¾è®¡
2. å¼€å‘æ´»è·ƒåº¦å’Œå›¢é˜Ÿåä½œ
3. æŠ€æœ¯æ ˆé€‰æ‹©å’Œä¾èµ–ç®¡ç†
4. å®‰å…¨æ€§å’Œæœ€ä½³å®è·µ
5. é¡¹ç›®å‘å±•è¶‹åŠ¿å’Œå»ºè®®

è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼ŒåŒ…å«å…·ä½“çš„æ•°æ®æ”¯æ’‘å’Œå¯è¡Œçš„æ”¹è¿›å»ºè®®ã€‚"""),
            HumanMessage(content="è¯·åˆ†æä»¥ä¸‹ä»“åº“æ•°æ®ï¼š\n{repo_data}")
        ])
        
        chain = prompt | self.llm
        response = await chain.ainvoke({"repo_data": json.dumps(repo_data, ensure_ascii=False, indent=2)})
        
        return {
            "type": "comprehensive",
            "analysis": response.content,
            "timestamp": datetime.now().isoformat(),
            "confidence": "high"
        }
    
    async def _security_analysis(self, repo_data: Dict[str, Any]) -> Dict[str, Any]:
        """å®‰å…¨æ€§åˆ†æ"""
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªç½‘ç»œå®‰å…¨ä¸“å®¶ï¼Œä¸“æ³¨äºä»£ç å®‰å…¨å®¡è®¡å’Œé£é™©è¯„ä¼°ã€‚

è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æä»“åº“çš„å®‰å…¨æ€§ï¼š
1. ä¾èµ–åŒ…å®‰å…¨é£é™©
2. ä»£ç ä¸­çš„æ½œåœ¨å®‰å…¨æ¼æ´
3. æ•æ„Ÿä¿¡æ¯æ³„éœ²é£é™©
4. è®¿é—®æ§åˆ¶å’Œæƒé™ç®¡ç†
5. å®‰å…¨æœ€ä½³å®è·µéµå¾ªæƒ…å†µ

æä¾›å…·ä½“çš„å®‰å…¨å»ºè®®å’Œä¿®å¤æ–¹æ¡ˆã€‚"""),
            HumanMessage(content="è¯·è¿›è¡Œå®‰å…¨åˆ†æï¼š\n{repo_data}")
        ])
        
        chain = prompt | self.llm
        response = await chain.ainvoke({"repo_data": json.dumps(repo_data, ensure_ascii=False, indent=2)})
        
        return {
            "type": "security",
            "analysis": response.content,
            "timestamp": datetime.now().isoformat(),
            "confidence": "high"
        }
    
    async def _performance_analysis(self, repo_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ€§èƒ½åˆ†æ"""
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªæ€§èƒ½ä¼˜åŒ–ä¸“å®¶ï¼Œä¸“æ³¨äºä»£ç æ€§èƒ½åˆ†æå’Œç³»ç»Ÿä¼˜åŒ–ã€‚

è¯·åˆ†æä»“åº“çš„æ€§èƒ½ç›¸å…³æ–¹é¢ï¼š
1. ä»£ç æ‰§è¡Œæ•ˆç‡
2. èµ„æºä½¿ç”¨ä¼˜åŒ–
3. æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½
4. ç¼“å­˜ç­–ç•¥
5. å¹¶å‘å’Œå¼‚æ­¥å¤„ç†

æä¾›å…·ä½“çš„æ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚"""),
            HumanMessage(content="è¯·è¿›è¡Œæ€§èƒ½åˆ†æï¼š\n{repo_data}")
        ])
        
        chain = prompt | self.llm
        response = await chain.ainvoke({"repo_data": json.dumps(repo_data, ensure_ascii=False, indent=2)})
        
        return {
            "type": "performance",
            "analysis": response.content,
            "timestamp": datetime.now().isoformat(),
            "confidence": "medium"
        }
    
    async def _quality_analysis(self, repo_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä»£ç è´¨é‡åˆ†æ"""
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªä»£ç è´¨é‡ä¸“å®¶ï¼Œä¸“æ³¨äºä»£ç è§„èŒƒå’Œæœ€ä½³å®è·µã€‚

è¯·åˆ†æä»£ç è´¨é‡ç›¸å…³æŒ‡æ ‡ï¼š
1. ä»£ç è§„èŒƒå’Œä¸€è‡´æ€§
2. æµ‹è¯•è¦†ç›–ç‡å’Œè´¨é‡
3. æ–‡æ¡£å®Œæ•´æ€§
4. ä»£ç å¯ç»´æŠ¤æ€§
5. é‡æ„å’ŒæŠ€æœ¯å€ºåŠ¡

æä¾›å…·ä½“çš„è´¨é‡æ”¹è¿›å»ºè®®ã€‚"""),
            HumanMessage(content="è¯·è¿›è¡Œè´¨é‡åˆ†æï¼š\n{repo_data}")
        ])
        
        chain = prompt | self.llm
        response = await chain.ainvoke({"repo_data": json.dumps(repo_data, ensure_ascii=False, indent=2)})
        
        return {
            "type": "quality",
            "analysis": response.content,
            "timestamp": datetime.now().isoformat(),
            "confidence": "high"
        }
    
    async def generate_smart_summary(
        self, 
        activities: List[Dict[str, Any]], 
        timeframe: str = "weekly"
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
        try:
            if not self.llm:
                return {"error": "AI æœåŠ¡ä¸å¯ç”¨"}
            
            # æ•°æ®é¢„å¤„ç†
            processed_data = self._preprocess_activities(activities, timeframe)
            
            prompt = ChatPromptTemplate.from_messages([
                SystemMessage(content=f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¼€å‘æ´»åŠ¨æ•°æ®ä¸­æå–å…³é”®æ´å¯Ÿã€‚

è¯·ä¸º {timeframe} çš„å¼€å‘æ´»åŠ¨ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼ŒåŒ…æ‹¬ï¼š
1. å…³é”®ç»Ÿè®¡æ•°æ®
2. é‡è¦è¶‹åŠ¿å’Œæ¨¡å¼
3. å¼‚å¸¸æ´»åŠ¨è¯†åˆ«
4. å›¢é˜Ÿè¡¨ç°äº®ç‚¹
5. éœ€è¦å…³æ³¨çš„é—®é¢˜

ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€æ€»ç»“ï¼Œçªå‡ºæœ€é‡è¦çš„ä¿¡æ¯ã€‚"""),
                HumanMessage(content="æ´»åŠ¨æ•°æ®ï¼š\n{activities}")
            ])
            
            chain = prompt | self.llm
            response = await chain.ainvoke({
                "activities": json.dumps(processed_data, ensure_ascii=False, indent=2)
            })
            
            return {
                "summary": response.content,
                "timeframe": timeframe,
                "activity_count": len(activities),
                "generated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return {"error": f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"}
    
    async def search_and_analyze(
        self, 
        query: str, 
        context_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """æœç´¢å¹¶åˆ†æç›¸å…³ä¿¡æ¯"""
        try:
            # ä½¿ç”¨æœç´¢å·¥å…·è·å–å¤–éƒ¨ä¿¡æ¯
            search_results = self.search_tool.run(query)
            
            prompt = ChatPromptTemplate.from_messages([
                SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯ç ”ç©¶å‘˜ï¼Œèƒ½å¤Ÿç»“åˆæœç´¢ç»“æœå’Œä¸Šä¸‹æ–‡æ•°æ®æä¾›æ·±å…¥åˆ†æã€‚

è¯·åŸºäºæœç´¢ç»“æœå’Œæä¾›çš„ä¸Šä¸‹æ–‡æ•°æ®ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜å¹¶æä¾›ç›¸å…³åˆ†æã€‚"""),
                HumanMessage(content="""
æœç´¢ç»“æœï¼š
{search_results}

ä¸Šä¸‹æ–‡æ•°æ®ï¼š
{context_data}

ç”¨æˆ·é—®é¢˜ï¼š{query}
""")
            ])
            
            chain = prompt | self.llm
            response = await chain.ainvoke({
                "search_results": search_results,
                "context_data": json.dumps(context_data or {}, ensure_ascii=False, indent=2),
                "query": query
            })
            
            return {
                "analysis": response.content,
                "search_results": search_results,
                "query": query,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ æœç´¢åˆ†æå¤±è´¥: {e}")
            return {"error": f"æœç´¢åˆ†æå¤±è´¥: {str(e)}"}
    
    def _format_context_data(self, context_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡æ•°æ®"""
        formatted = []
        
        if "repository" in context_data:
            repo = context_data["repository"]
            formatted.append(f"ä»“åº“: {repo.get('name', 'Unknown')}")
            formatted.append(f"æè¿°: {repo.get('description', 'N/A')}")
            formatted.append(f"ä¸»è¦è¯­è¨€: {repo.get('language', 'N/A')}")
            formatted.append(f"Stars: {repo.get('stargazers_count', 0)}")
        
        if "recent_activities" in context_data:
            activities = context_data["recent_activities"]
            formatted.append(f"\næœ€è¿‘æ´»åŠ¨ ({len(activities)} é¡¹):")
            for i, activity in enumerate(activities[:5]):
                formatted.append(f"  {i+1}. {activity.get('type', 'unknown')}: {activity.get('title', 'N/A')[:50]}...")
        
        if "statistics" in context_data:
            stats = context_data["statistics"]
            formatted.append(f"\nç»Ÿè®¡æ•°æ®:")
            for key, value in stats.items():
                formatted.append(f"  {key}: {value}")
        
        return "\n".join(formatted)
    
    def _preprocess_activities(
        self, 
        activities: List[Dict[str, Any]], 
        timeframe: str
    ) -> Dict[str, Any]:
        """é¢„å¤„ç†æ´»åŠ¨æ•°æ®"""
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = {}
        by_author = {}
        by_date = {}
        
        for activity in activities:
            # æŒ‰ç±»å‹åˆ†ç»„
            activity_type = activity.get("activity_type", "unknown")
            if activity_type not in by_type:
                by_type[activity_type] = []
            by_type[activity_type].append(activity)
            
            # æŒ‰ä½œè€…åˆ†ç»„
            author = activity.get("author_login", "unknown")
            if author not in by_author:
                by_author[author] = []
            by_author[author].append(activity)
            
            # æŒ‰æ—¥æœŸåˆ†ç»„
            created_at = activity.get("github_created_at", "")
            if created_at:
                date_key = created_at[:10]  # YYYY-MM-DD
                if date_key not in by_date:
                    by_date[date_key] = []
                by_date[date_key].append(activity)
        
        return {
            "total_activities": len(activities),
            "timeframe": timeframe,
            "by_type": {k: len(v) for k, v in by_type.items()},
            "by_author": {k: len(v) for k, v in by_author.items()},
            "by_date": {k: len(v) for k, v in by_date.items()},
            "top_contributors": sorted(by_author.items(), key=lambda x: len(x[1]), reverse=True)[:5],
            "most_active_dates": sorted(by_date.items(), key=lambda x: len(x[1]), reverse=True)[:5]
        }
    
    async def clear_conversation(self, user_id: str) -> bool:
        """æ¸…é™¤ç”¨æˆ·å¯¹è¯å†å²"""
        try:
            if user_id in self.conversation_chains:
                del self.conversation_chains[user_id]
                logger.info(f"âœ… æ¸…é™¤ç”¨æˆ·å¯¹è¯å†å² - ç”¨æˆ·: {user_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"ğŸ’¥ æ¸…é™¤å¯¹è¯å†å²å¤±è´¥: {e}")
            return False
    
    def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            "llm_available": self.llm is not None,
            "model": self.ai_config.openai_model if self.llm else None,
            "active_conversations": len(self.conversation_chains),
            "last_updated": datetime.now().isoformat()
        }